{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AV_Logo.png\" style=\"width: 200px;height: 75px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:\n",
    "------------\n",
    "* [Ensemble Model - example and intuition](#Ensemble-Model---example-and-intuition)\n",
    "* [What is Ensemble Modeling?](#What-is-Ensemble-Modeling?)\n",
    "* [Ensemble Techniques](#Ensemble-Techniques)\n",
    "* [Advantages and Disadvantages of ensemble models](#Advantages-and-Disadvantages-of-ensemble-models)\n",
    "* [Implementation of Ensemble modeling](#Implementation-of-Ensemble-modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model - example and intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try to understand ensemble model using an example. Suppose we are trying to solve a classification challenge. The problem is to set the rules for classification of spam emails.\n",
    "\n",
    "What we can do is that we generate various rules for classification of spam emails. \n",
    "\n",
    "Let’s look at the some of them:\n",
    "\n",
    "**Spam**\n",
    "* Have length less than 20 words\n",
    "* Have only images (promotional images)\n",
    "* Have specific key words like “make money and grow” and “reduce your fat”\n",
    "* More miss spelled words in the email\n",
    "\n",
    "**Not Spam**\n",
    "* Email from Analytics Vidhya domain\n",
    "* Email from family members or anyone from e-mail address book\n",
    "\n",
    "Listed here are some common rules for filtering the SPAM e-mails. Do you think that all these rules individually can predict the correct class?\n",
    "\n",
    "Most of us would say no – And that’s true! Combining these rules will provide robust prediction as compared to prediction done by individual rules. This is the principle of Ensemble Modeling. Ensemble model combines multiple ‘individual’ (diverse) models together and delivers superior prediction power.\n",
    "\n",
    "If you want to relate this to real life, a group of people are likely to make better decisions compared to individuals, especially when group members come from diverse background. The same is true with machine learning. Basically, an ensemble is a supervised learning technique for combining multiple weak learners/ models to produce a strong learner. Ensemble model works better, when we ensemble models with low correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Ensemble Modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, ensembling is a technique of combining two or more algorithms of similar or dissimilar types called base learners. This is done to make a more robust system which incorporates the predictions from all the base learners. It can be understood as conference room meeting between multiple traders to make a decision on whether the price of a stock will go up or not.\n",
    "\n",
    "Since all of them have a different understanding of the stock market and thus a different mapping function from the problem statement to the desired outcome. Therefore, they are supposed to make varied predictions on the stock price based on their own understandings of the market.\n",
    "\n",
    "Now we can take all of these predictions into account while making the final decision. This will make our final decision more robust, accurate and less likely to be biased. The final decision might have been different if one of these traders would have made this decision alone.\n",
    "\n",
    "You can consider another example of a candidate going through multiple rounds of job interviews. The final decision of candidate’s ability is generally taken based on the feedback of all the interviewers. Although a single interviewer might not be able to test the candidate for each required skill and trait. But the combined feedback of multiple interviewers usually helps in better assessment of the candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble techniques can be as simple and averaging and can go on to more complicated bagging and stacking. We'll see these as we go further today. \n",
    "\n",
    "**Averaging** - Averaging works well for a wide range of problems (both classification and regression). We dont have to worry much about averaging as it is simply taking the mean of individual model predictions. Averaging predictions often reduces overfit. You ideally want a smooth separation between classes, and a single model’s predictions can be a little rough around the edges. Basically all we're doing is making multiple models of the same or different type and averaging their results for a final output. For example - \n",
    "\n",
    "<img src=\"average.png\" style=\"width: 300px;height: 50px\">\n",
    "\n",
    "\n",
    "**Weighted Averaging** - When we simply average the results from different models, we assume all the predictions and models as equally stable and confident. However that's not the case. Some models might be more stable and show more accuracy as compared to the others. What we can do is we can give more weightage to the  more stable predictions as compared to the less stable ones resulting in a combined output which is more stable and confident as compared to simple averaging.\n",
    "\n",
    "<img src=\"waverage2.png\" style=\"width: 350px;height: 80px\">\n",
    "\n",
    "\n",
    "**Voting** - This is mostly used during classification problems. In this case multiple classification algorithms are made. Therefore we have multiple predictions for each instance. The final prediction is the one which recieves more than half of the votes. If none of the predictions get more than half of the votes, we generally take the most voted prediction. \n",
    "\n",
    "<img src=\"voting.png\" style=\"width: 400px;height: 50px\">\n",
    "\n",
    "**Weighted Voting** - Similar to weighted averaging we can give weights to different predictions here as well. The more stable predictions have more say towards the final predictions as compared the less confident predictions. Due to more stability weighted voting is preferred over normal voting.\n",
    "\n",
    "<img src=\"wvoting.png\" style=\"width: 400px;height: 80px\">\n",
    "\n",
    "\n",
    "**Bagging** - Bagging is also referred to as bootstrap aggregation. To understand bagging, we first need to understand bootstrapping. Bootstrapping is a sampling technique in which we choose ‘n’ observations or rows out of the original dataset of ‘n’ rows as well. But the key is that each row is selected with replacement from the original dataset so that each row is equally likely to be selected in each iteration. Let’s say we have 3 rows numbered 1, 2 and 3.\n",
    "\n",
    "<img src=\"bagging1.png\" style=\"width:250px;height: 100px\">\n",
    "\n",
    "For bootstrapped sample, we choose one out of these three randomly. Say we chose Row 2\n",
    "\n",
    "<img src=\"bagging2.png\" style=\"width: 300px;height: 100px\">\n",
    "\n",
    "You see that even though Row 2 is chosen from the data to the bootstrap sample, it’s still present in the data. Now, each of the three:\n",
    "\n",
    "<img src=\"bagging3.png\" style=\"width: 300px;height: 100px\">\n",
    "\n",
    "Rows have the same probability of being selected again. Let’s say we choose Row 1 this time.\n",
    "\n",
    "Again, each row in the data has the same probability to be chosen for Bootstrapped sample. Let’s say we randomly choose Row 1 again.\n",
    "\n",
    "<img src=\"bagging4.png\" style=\"width: 300px;height: 100px\">\n",
    "\n",
    "Thus, we can have multiple bootstrapped samples from the same data. Once we have these multiple bootstrapped samples, we can apply the algorithm for each of these bootstrapped samples and use the majority vote or averaging concepts to get the final prediction. This is how bagging works.\n",
    "\n",
    "One important thing to note here is that it’s done mainly to reduce the variance.\n",
    "\n",
    "There are many more techniques which can be used while ensembling models. Boosting and Stacking being the commonly used ones. You can read about them [here](https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages of ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "\n",
    "Below are the advantages of ensemble models - \n",
    "\n",
    "* Ensembling is a proven method for improving the accuracy of the model and works in most of the cases.\n",
    "* It is the key ingredient for winning almost all of the machine learning hackathons.\n",
    "* Ensembling makes the model more robust and stable thus ensuring decent performance on the test cases in most scenarios.\n",
    "* You can use ensembling to capture linear and simple as well non-linear complex relationships in the data. This can be done by using two different models and forming an ensemble of two.\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "Below are the disadvantages of ensemble\n",
    "\n",
    "* Ensembling reduces the model interpretability and makes it very difficult to draw any crucial business insights at the end.\n",
    "* It is time-consuming and thus might not be the best idea for real-time applications.\n",
    "* The selection of models for creating an ensemble is an art which is really hard to master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble techniques are being used in every DataHack Problem. Choosing the right ensembles is more of an art than straight forward science. With experience, you will develop a knack of which ensemble learner to use in different kinds of scenario and base learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Ensemble modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code for Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('winequality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W0001</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W0002</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W0003</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W0004</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W0005</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0  W0001            7.0              0.27         0.36            20.7   \n",
       "1  W0002            6.3              0.30         0.34             1.6   \n",
       "2  W0003            8.1              0.28         0.40             6.9   \n",
       "3  W0004            7.2              0.23         0.32             8.5   \n",
       "4  W0005            7.2              0.23         0.32             8.5   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.045                 45.0                 170.0   1.0010  3.00   \n",
       "1      0.049                 14.0                 132.0   0.9940  3.30   \n",
       "2      0.050                 30.0                  97.0   0.9951  3.26   \n",
       "3      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "4      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "\n",
       "   sulphates  alcohol  quality  \n",
       "0       0.45      8.8        2  \n",
       "1        NaN      9.5        2  \n",
       "2        NaN     10.1        2  \n",
       "3       0.40      9.9        2  \n",
       "4       0.40      9.9        2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.loc[data.quality == 2, 'quality'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first separate dependent and independent variables\n",
    "X = data.drop(['ID', 'quality'], axis=1)\n",
    "y = data.quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>8.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>10.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>9.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>9.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>10.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.334031</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>9.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>8.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>28.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.99380</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.033</td>\n",
       "      <td>11.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.99080</td>\n",
       "      <td>2.990000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.6</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.035</td>\n",
       "      <td>17.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.99470</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>9.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.040</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.99200</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>10.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>48.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.99120</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>12.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.3</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>19.25</td>\n",
       "      <td>0.040</td>\n",
       "      <td>41.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1.00020</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>9.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.032</td>\n",
       "      <td>28.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.99140</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.99280</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>9.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.029</td>\n",
       "      <td>29.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.98920</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.033</td>\n",
       "      <td>17.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.99170</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>11.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>34.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.029</td>\n",
       "      <td>29.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.98920</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.038</td>\n",
       "      <td>19.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99120</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.049</td>\n",
       "      <td>41.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>10.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.074</td>\n",
       "      <td>25.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99370</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>9.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.334031</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.052</td>\n",
       "      <td>16.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.420000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.046</td>\n",
       "      <td>56.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.052</td>\n",
       "      <td>35.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.051</td>\n",
       "      <td>32.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.99610</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>10.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.047</td>\n",
       "      <td>17.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99140</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>11.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.334031</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.033</td>\n",
       "      <td>37.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.99060</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>12.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>5.8</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.046</td>\n",
       "      <td>42.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.99324</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>10.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>10.10</td>\n",
       "      <td>0.032</td>\n",
       "      <td>8.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.99626</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.334031</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.021</td>\n",
       "      <td>29.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99188</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>11.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.015</td>\n",
       "      <td>20.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98970</td>\n",
       "      <td>3.370000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>12.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4872</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>12.40</td>\n",
       "      <td>0.032</td>\n",
       "      <td>50.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.99622</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>9.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.334031</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.030</td>\n",
       "      <td>33.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.99044</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>11.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4874</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.048</td>\n",
       "      <td>16.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.99282</td>\n",
       "      <td>3.490000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.035</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99245</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>9.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4876</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.038</td>\n",
       "      <td>34.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.99132</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>11.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4877</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.032</td>\n",
       "      <td>12.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.99286</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>8.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4878</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.035</td>\n",
       "      <td>6.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.99234</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4879</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.334031</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>9.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4881</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.030</td>\n",
       "      <td>34.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.070000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4882</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.037</td>\n",
       "      <td>45.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.99184</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>10.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4883</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.035</td>\n",
       "      <td>60.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.98964</td>\n",
       "      <td>3.270000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>11.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.048</td>\n",
       "      <td>68.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.99492</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.334031</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.028</td>\n",
       "      <td>45.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.99168</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>12.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.023</td>\n",
       "      <td>5.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.98928</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.052</td>\n",
       "      <td>38.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.99330</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>9.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.030</td>\n",
       "      <td>34.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.070000</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.036</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.98938</td>\n",
       "      <td>3.060000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>11.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.038</td>\n",
       "      <td>38.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.99074</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>10.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.032</td>\n",
       "      <td>29.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.99298</td>\n",
       "      <td>3.290000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>9.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.270000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.334031</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.188762</td>\n",
       "      <td>0.490158</td>\n",
       "      <td>9.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.990000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.340000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>11.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0             0.270     0.360000           20.70      0.045   \n",
       "1               6.3             0.300     0.340000            1.60      0.049   \n",
       "2               8.1             0.280     0.400000            6.90      0.050   \n",
       "3               7.2             0.230     0.320000            8.50      0.058   \n",
       "4               7.2             0.230     0.320000            8.50      0.058   \n",
       "5               8.1             0.280     0.400000            6.90      0.050   \n",
       "6               6.2             0.320     0.334031            7.00      0.045   \n",
       "7               7.0             0.270     0.360000           20.70      0.045   \n",
       "8               6.3             0.300     0.340000            1.60      0.049   \n",
       "9               8.1             0.220     0.430000            1.50      0.044   \n",
       "10              8.1             0.270     0.410000            1.45      0.033   \n",
       "11              8.6             0.230     0.400000            4.20      0.035   \n",
       "12              7.9             0.180     0.370000            1.20      0.040   \n",
       "13              6.6             0.160     0.400000            1.50      0.044   \n",
       "14              8.3             0.420     0.620000           19.25      0.040   \n",
       "15              6.6             0.170     0.380000            1.50      0.032   \n",
       "16              6.3             0.480     0.040000            1.10      0.046   \n",
       "17              6.2             0.660     0.480000            1.20      0.029   \n",
       "18              7.4             0.340     0.420000            1.10      0.033   \n",
       "19              6.5             0.310     0.140000            7.50      0.044   \n",
       "20              6.2             0.660     0.480000            1.20      0.029   \n",
       "21              6.4             0.310     0.380000            2.90      0.038   \n",
       "22              6.8             0.260     0.420000            1.70      0.049   \n",
       "23              7.6             0.670     0.140000            1.50      0.074   \n",
       "24              6.6             0.270     0.334031            1.30      0.052   \n",
       "25              7.0             0.250     0.320000            9.00      0.046   \n",
       "26              6.9             0.240     0.350000            1.00      0.052   \n",
       "27              7.0             0.280     0.390000            8.70      0.051   \n",
       "28              7.4             0.270     0.480000            1.10      0.047   \n",
       "29              7.2             0.320     0.334031            2.00      0.033   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4868            5.8             0.230     0.310000            4.50      0.046   \n",
       "4869            6.6             0.240     0.330000           10.10      0.032   \n",
       "4870            6.1             0.320     0.334031            6.60      0.021   \n",
       "4871            5.0             0.200     0.400000            1.90      0.015   \n",
       "4872            6.0             0.420     0.410000           12.40      0.032   \n",
       "4873            5.7             0.210     0.334031            1.60      0.030   \n",
       "4874            5.6             0.200     0.360000            2.50      0.048   \n",
       "4875            7.4             0.220     0.260000            1.20      0.035   \n",
       "4876            6.2             0.380     0.420000            2.50      0.038   \n",
       "4877            5.9             0.540     0.000000            0.80      0.032   \n",
       "4878            6.2             0.530     0.020000            0.90      0.035   \n",
       "4879            6.6             0.340     0.334031            8.10      0.046   \n",
       "4880            6.6             0.340     0.400000            8.10      0.046   \n",
       "4881            5.0             0.235     0.270000           11.75      0.030   \n",
       "4882            5.5             0.320     0.130000            1.30      0.037   \n",
       "4883            4.9             0.470     0.170000            1.90      0.035   \n",
       "4884            6.5             0.330     0.380000            8.30      0.048   \n",
       "4885            6.6             0.340     0.400000            8.10      0.046   \n",
       "4886            6.2             0.210     0.334031            5.70      0.028   \n",
       "4887            6.2             0.410     0.220000            1.90      0.023   \n",
       "4888            6.8             0.220     0.360000            1.20      0.052   \n",
       "4889            4.9             0.235     0.270000           11.75      0.030   \n",
       "4890            6.1             0.340     0.290000            2.20      0.036   \n",
       "4891            5.7             0.210     0.320000            0.90      0.038   \n",
       "4892            6.5             0.230     0.380000            1.30      0.032   \n",
       "4893            6.2             0.210     0.290000            1.60      0.039   \n",
       "4894            6.6             0.320     0.334031            8.00      0.047   \n",
       "4895            6.5             0.240     0.190000            1.20      0.041   \n",
       "4896            5.5             0.290     0.300000            1.10      0.022   \n",
       "4897            6.0             0.210     0.380000            0.80      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density        pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.000000   0.450000   \n",
       "1                    14.0                 132.0  0.99400  3.300000   0.490158   \n",
       "2                    30.0                  97.0  0.99510  3.260000   0.490158   \n",
       "3                    47.0                 186.0  0.99560  3.190000   0.400000   \n",
       "4                    47.0                 186.0  0.99560  3.190000   0.400000   \n",
       "5                    30.0                  97.0  0.99510  3.260000   0.440000   \n",
       "6                    30.0                 136.0  0.99490  3.188762   0.470000   \n",
       "7                    45.0                 170.0  1.00100  3.000000   0.450000   \n",
       "8                    14.0                 132.0  0.99400  3.300000   0.490158   \n",
       "9                    28.0                 129.0  0.99380  3.220000   0.450000   \n",
       "10                   11.0                  63.0  0.99080  2.990000   0.560000   \n",
       "11                   17.0                 109.0  0.99470  3.188762   0.530000   \n",
       "12                   16.0                  75.0  0.99200  3.180000   0.630000   \n",
       "13                   48.0                 143.0  0.99120  3.188762   0.520000   \n",
       "14                   41.0                 172.0  1.00020  2.980000   0.670000   \n",
       "15                   28.0                 112.0  0.99140  3.250000   0.550000   \n",
       "16                   30.0                  99.0  0.99280  3.240000   0.360000   \n",
       "17                   29.0                  75.0  0.98920  3.330000   0.390000   \n",
       "18                   17.0                 171.0  0.99170  3.120000   0.530000   \n",
       "19                   34.0                 133.0  0.99550  3.220000   0.500000   \n",
       "20                   29.0                  75.0  0.98920  3.330000   0.390000   \n",
       "21                   19.0                 102.0  0.99120  3.188762   0.350000   \n",
       "22                   41.0                 122.0  0.99300  3.470000   0.480000   \n",
       "23                   25.0                 168.0  0.99370  3.050000   0.490158   \n",
       "24                   16.0                 142.0  0.99510  3.420000   0.470000   \n",
       "25                   56.0                 245.0  0.99550  3.250000   0.500000   \n",
       "26                   35.0                 146.0  0.99300  3.450000   0.440000   \n",
       "27                   32.0                 141.0  0.99610  3.188762   0.530000   \n",
       "28                   17.0                 132.0  0.99140  3.190000   0.490000   \n",
       "29                   37.0                 114.0  0.99060  3.100000   0.710000   \n",
       "...                   ...                   ...      ...       ...        ...   \n",
       "4868                 42.0                 124.0  0.99324  3.188762   0.490158   \n",
       "4869                  8.0                  81.0  0.99626  3.188762   0.510000   \n",
       "4870                 29.0                 132.0  0.99188  3.188762   0.360000   \n",
       "4871                 20.0                  98.0  0.98970  3.370000   0.550000   \n",
       "4872                 50.0                 179.0  0.99622  3.140000   0.600000   \n",
       "4873                 33.0                 122.0  0.99044  3.188762   0.520000   \n",
       "4874                 16.0                 125.0  0.99282  3.490000   0.490000   \n",
       "4875                 18.0                  97.0  0.99245  3.120000   0.490158   \n",
       "4876                 34.0                 117.0  0.99132  3.360000   0.590000   \n",
       "4877                 12.0                  82.0  0.99286  3.250000   0.360000   \n",
       "4878                  6.0                  81.0  0.99234  3.240000   0.350000   \n",
       "4879                 68.0                 170.0  0.99494  3.150000   0.500000   \n",
       "4880                 68.0                 170.0  0.99494  3.150000   0.490158   \n",
       "4881                 34.0                 118.0  0.99540  3.070000   0.500000   \n",
       "4882                 45.0                 156.0  0.99184  3.260000   0.490158   \n",
       "4883                 60.0                 148.0  0.98964  3.270000   0.350000   \n",
       "4884                 68.0                 174.0  0.99492  3.140000   0.500000   \n",
       "4885                 68.0                 170.0  0.99494  3.150000   0.500000   \n",
       "4886                 45.0                 121.0  0.99168  3.210000   1.080000   \n",
       "4887                  5.0                  56.0  0.98928  3.040000   0.490158   \n",
       "4888                 38.0                 127.0  0.99330  3.188762   0.540000   \n",
       "4889                 34.0                 118.0  0.99540  3.070000   0.490158   \n",
       "4890                 25.0                 100.0  0.98938  3.060000   0.440000   \n",
       "4891                 38.0                 121.0  0.99074  3.240000   0.460000   \n",
       "4892                 29.0                 112.0  0.99298  3.290000   0.540000   \n",
       "4893                 24.0                  92.0  0.99114  3.270000   0.500000   \n",
       "4894                 57.0                 168.0  0.99490  3.188762   0.490158   \n",
       "4895                 30.0                 111.0  0.99254  2.990000   0.460000   \n",
       "4896                 20.0                 110.0  0.98869  3.340000   0.380000   \n",
       "4897                 22.0                  98.0  0.98941  3.260000   0.320000   \n",
       "\n",
       "        alcohol  \n",
       "0      8.800000  \n",
       "1      9.500000  \n",
       "2     10.100000  \n",
       "3      9.900000  \n",
       "4      9.900000  \n",
       "5     10.100000  \n",
       "6      9.600000  \n",
       "7      8.800000  \n",
       "8      9.500000  \n",
       "9     11.000000  \n",
       "10    12.000000  \n",
       "11     9.700000  \n",
       "12    10.800000  \n",
       "13    12.400000  \n",
       "14     9.700000  \n",
       "15    11.400000  \n",
       "16     9.600000  \n",
       "17    12.800000  \n",
       "18    11.300000  \n",
       "19     9.500000  \n",
       "20    12.800000  \n",
       "21    11.000000  \n",
       "22    10.500000  \n",
       "23     9.300000  \n",
       "24    10.000000  \n",
       "25    10.400000  \n",
       "26    10.000000  \n",
       "27    10.500000  \n",
       "28    11.600000  \n",
       "29    12.300000  \n",
       "...         ...  \n",
       "4868  10.800000  \n",
       "4869   9.800000  \n",
       "4870  11.450000  \n",
       "4871  12.050000  \n",
       "4872   9.700000  \n",
       "4873  11.900000  \n",
       "4874  10.000000  \n",
       "4875   9.700000  \n",
       "4876  11.600000  \n",
       "4877   8.800000  \n",
       "4878   9.500000  \n",
       "4879   9.533333  \n",
       "4880   9.533333  \n",
       "4881   9.400000  \n",
       "4882  10.700000  \n",
       "4883  11.500000  \n",
       "4884   9.600000  \n",
       "4885   9.550000  \n",
       "4886  12.150000  \n",
       "4887  13.000000  \n",
       "4888   9.200000  \n",
       "4889   9.400000  \n",
       "4890  11.800000  \n",
       "4891  10.600000  \n",
       "4892   9.700000  \n",
       "4893  11.200000  \n",
       "4894   9.600000  \n",
       "4895   9.400000  \n",
       "4896  12.800000  \n",
       "4897  11.800000  \n",
       "\n",
       "[4898 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill missing values\n",
    "X.fillna(X.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "nb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75977172513437852"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, nb.predict_proba(X)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logReg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7966601910494242"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, logReg.predict_proba(X)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = (nb.predict_proba(X)[:, 1] + logReg.predict_proba(X)[:, 1]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78461535582206665"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code for Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = nb.predict_proba(X)[:, 1]*0.1 + logReg.predict_proba(X)[:, 1] * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79807696626690017"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code for Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vt = VotingClassifier([('nb', nb), ('logReg', logReg)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nb', GaussianNB(priors=None)), ('logReg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))],\n",
       "         n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78461535582206665"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, vt.predict_proba(X)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code for Weighted Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vt = VotingClassifier([('nb', nb), ('logReg', logReg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nb', GaussianNB(priors=None)), ('logReg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))],\n",
       "         n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes score: 0.70641077991\n",
      "logReg score: 0.750918742344\n",
      "ensemble score: 0.738260514496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print ('naive bayes score:', accuracy_score(y, nb.predict(X)))\n",
    "print ('logReg score:', accuracy_score(y, logReg.predict(X)))\n",
    "print ('ensemble score:', accuracy_score(y, vt.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vt = VotingClassifier([('nb', nb), ('logReg', logReg)], weights=[0.1, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nb', GaussianNB(priors=None)), ('logReg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))],\n",
       "         n_jobs=1, voting='hard', weights=[0.1, 0.9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes score: 0.70641077991\n",
      "logReg score: 0.750918742344\n",
      "ensemble score: 0.750918742344\n"
     ]
    }
   ],
   "source": [
    "print ('naive bayes score:', accuracy_score(y, nb.predict(X)))\n",
    "print ('logReg score:', accuracy_score(y, logReg.predict(X)))\n",
    "print ('ensemble score:', accuracy_score(y, vt.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code for Bagging (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmf = RandomForestClassifier(n_estimators=100, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8817269685127791"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, rmf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "Q1. Apply your learnings on [Loan Prediction practice problem](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/) \n",
    "\n",
    "Q2. Apply your learnings on [Big Mart Sales practice problem](https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for today!\n",
    "----------------\n",
    "-------------------------------\n",
    "<img src=\"AV_Datafest_logo.png\" style=\"width: 200px;height: 200px\"/>\n",
    "[www.analyticsvidhya.com](www.analyticsvidhya.com)\n",
    "\n",
    "DATAFEST 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
